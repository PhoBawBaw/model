{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcAsyaCFbWFc"
   },
   "source": [
    "## 전체 학습 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4361,
     "status": "ok",
     "timestamp": 1724778699874,
     "user": {
      "displayName": "김태영",
      "userId": "13282995900780911917"
     },
     "user_tz": -540
    },
    "id": "OnLBqsHon9Zf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class AudioFolderDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, target_sample_rate=8000, target_length=8000):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "        self.label_map = {}\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.target_length = target_length\n",
    "\n",
    "        self._load_dataset()\n",
    "\n",
    "    def _load_dataset(self):\n",
    "        label_names = os.listdir(self.root_dir)\n",
    "        for idx, label_name in enumerate(label_names):\n",
    "            label_dir = os.path.join(self.root_dir, label_name)\n",
    "            if os.path.isdir(label_dir):\n",
    "                self.label_map[label_name] = idx\n",
    "                for file_name in os.listdir(label_dir):\n",
    "                    if file_name.endswith('.wav'):\n",
    "                        self.file_paths.append(os.path.join(label_dir, file_name))\n",
    "                        self.labels.append(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # 오디오 파일 로드\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        \n",
    "        # 항상 모노로 변환\n",
    "        if waveform.shape[0] > 1:  # 스테레오(2 채널)일 경우\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "        # 전처리 함수 적용\n",
    "        waveform = self.preprocess(waveform, sample_rate)\n",
    "        \n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "        \n",
    "        return waveform, label\n",
    "\n",
    "    def preprocess(self, waveform, sample_rate):\n",
    "        # 리샘플링\n",
    "        if sample_rate != self.target_sample_rate:\n",
    "            transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.target_sample_rate)\n",
    "            waveform = transform(waveform)\n",
    "\n",
    "        # 길이 맞추기\n",
    "        waveform = self.pad_or_trim_waveform(waveform)\n",
    "        \n",
    "        return waveform\n",
    "\n",
    "    def pad_or_trim_waveform(self, waveform):\n",
    "        current_length = waveform.shape[1]\n",
    "        if current_length > self.target_length:\n",
    "            # 잘라내기\n",
    "            waveform = waveform[:, :self.target_length]\n",
    "        elif current_length < self.target_length:\n",
    "            # 패딩\n",
    "            pad_length = self.target_length - current_length\n",
    "            waveform = F.pad(waveform, (0, pad_length))\n",
    "        \n",
    "        return waveform\n",
    "\n",
    "# CNN 모델 정의\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, input_shape, num_label):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.norm_layer = nn.BatchNorm2d(1)\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # CNN 레이어 이후 출력 크기 계산\n",
    "        with torch.no_grad():\n",
    "            sample_input = torch.zeros(1, *input_shape).unsqueeze(0)\n",
    "            sample_output = self._forward_features(sample_input)\n",
    "            flatten_size = sample_output.numel()\n",
    "\n",
    "        self.fc1 = nn.Linear(flatten_size, 128)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, num_label)\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = F.interpolate(x, size=(32, 32), mode='bilinear', align_corners=False)\n",
    "        x = self.norm_layer(x)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout1(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 데이터 준비\n",
    "data_dir = './dataset/'  # 데이터 폴더 경로\n",
    "# dataset = AudioFolderDataset(data_dir, transform=preprocess)\n",
    "dataset = AudioFolderDataset(root_dir=data_dir, target_sample_rate=8000, target_length=8000)\n",
    "\n",
    "# 시드 고정\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ayet-kld0Rmx"
   },
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "input_shape = (1, 8000)  # 입력 크기를 (채널, 길이)로 설정\n",
    "num_label = len(dataset.label_map)\n",
    "model = CNNClassifier(input_shape, num_label)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 모델 훈련 및 검증\n",
    "num_epochs = 150\n",
    "best_val_accuracy = 0.0\n",
    "best_model_path = './model/model_1.pth'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.unsqueeze(1)  # (batch_size, 1, 8000) 형태로 변환\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "    # 검증 루프\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.unsqueeze(1)  # (batch_size, 1, 8000) 형태로 변환\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f'Validation Accuracy: {val_accuracy}')\n",
    "\n",
    "    # 최적 모델 저장\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f'Saved Best Model with Accuracy: {val_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLYVWyH1bRpG"
   },
   "source": [
    "## Test 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 841,
     "status": "ok",
     "timestamp": 1724646146265,
     "user": {
      "displayName": "김태영",
      "userId": "13282995900780911917"
     },
     "user_tz": -540
    },
    "id": "M08hcSqZaAi0",
    "outputId": "d7a59c80-836a-4d53-dbd9-e1620563dde7"
   },
   "outputs": [],
   "source": [
    "best_model_path = './model/model_1.pth'\n",
    "# 이후 필요할 때 테스트 셋 선언\n",
    "\n",
    "# 모델 초기화\n",
    "input_shape = (1, 8000)  # 입력 크기를 (채널, 길이)로 설정\n",
    "num_label = len(dataset.label_map)\n",
    "print(dataset.label_map)\n",
    "model = CNNClassifier(input_shape, num_label)\n",
    "# 모델 평가\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.unsqueeze(1)  # (batch_size, 1, 8000) 형태로 변환\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOZEUwXXvjOK142byrF1kWj",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv37",
   "language": "python",
   "name": "venv37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
