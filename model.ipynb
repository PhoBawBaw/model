{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMLbDM9ZtcHTiQ1wXfVtmON"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l6UDcGNdtujh","executionInfo":{"status":"ok","timestamp":1723647343110,"user_tz":-540,"elapsed":39538,"user":{"displayName":"김태영","userId":"13282995900780911917"}},"outputId":"5b09231a-4290-4eee-e696-f1ff38a08824"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":382},"id":"VrfB_kRytIw6","executionInfo":{"status":"error","timestamp":1723647360919,"user_tz":-540,"elapsed":9452,"user":{"displayName":"김태영","userId":"13282995900780911917"}},"outputId":"be890d05-a2c0-440c-9f5f-294cdc5a2757"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [1, 32, 32] and output size of (32, 32). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-247cac8b07ad>\u001b[0m in \u001b[0;36m<cell line: 102>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 입력 크기를 (채널, 높이, 너비)로 설정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0mnum_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNNClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-247cac8b07ad>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_shape, num_label)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0msample_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0msample_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mflatten_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-247cac8b07ad>\u001b[0m in \u001b[0;36m_forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/upsampling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         return F.interpolate(input, self.size, self.scale_factor, self.mode, self.align_corners,\n\u001b[0m\u001b[1;32m    158\u001b[0m                              recompute_scale_factor=self.recompute_scale_factor)\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   3959\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3960\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3961\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   3962\u001b[0m                     \u001b[0;34m\"Input and output must have the same number of spatial dimensions, but got \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3963\u001b[0m                     \u001b[0;34mf\"input with spatial dimensions of {list(input.shape[2:])} and output size of {size}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [1, 32, 32] and output size of (32, 32). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format."]}],"source":["import os\n","import torch\n","import torchaudio\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from sklearn.metrics import accuracy_score\n","\n","# 데이터셋 클래스 정의\n","class AudioFolderDataset(Dataset):\n","    def __init__(self, root_dir, transform=None):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.file_paths = []\n","        self.labels = []\n","        self.label_map = {}\n","\n","        self._load_dataset()\n","    def _load_dataset(self):\n","        label_names = os.listdir(self.root_dir)\n","        for idx, label_name in enumerate(label_names):\n","            label_dir = os.path.join(self.root_dir, label_name)\n","            if os.path.isdir(label_dir):\n","                self.label_map[label_name] = idx\n","                for file_name in os.listdir(label_dir):\n","                    if file_name.endswith('.wav'):\n","                        self.file_paths.append(os.path.join(label_dir, file_name))\n","                        self.labels.append(idx)\n","\n","    def __len__(self):\n","        return len(self.file_paths)\n","\n","    def __getitem__(self, idx):\n","        file_path = self.file_paths[idx]\n","        label = self.labels[idx]\n","        waveform, sample_rate = torchaudio.load(file_path)\n","        if self.transform:\n","            waveform = self.transform(waveform)\n","        return waveform, label\n","\n","# 전처리 함수 정의\n","def preprocess(waveform, new_sample_rate=8000):\n","    transform = torchaudio.transforms.Resample(orig_freq=waveform.shape[1], new_freq=new_sample_rate)\n","    return transform(waveform)\n","\n","# 모델 정의\n","class CNNClassifier(nn.Module):\n","    def __init__(self, input_shape, num_label):\n","        super(CNNClassifier, self).__init__()\n","        self.norm_layer = nn.BatchNorm2d(1)\n","        self.resizing = nn.Upsample(size=(32, 32), mode='bilinear', align_corners=False)\n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.dropout1 = nn.Dropout(0.25)\n","        self.flatten = nn.Flatten()\n","\n","        # CNN 레이어 이후 출력 크기 계산\n","        with torch.no_grad():\n","            sample_input = torch.zeros(1, *input_shape).unsqueeze(0)\n","            sample_output = self._forward_features(sample_input)\n","            flatten_size = sample_output.numel()\n","\n","        self.fc1 = nn.Linear(flatten_size, 128)\n","        self.dropout2 = nn.Dropout(0.5)\n","        self.fc2 = nn.Linear(128, num_label)\n","\n","    def _forward_features(self, x):\n","        x = self.resizing(x)\n","        x = self.norm_layer(x)\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = self.pool(x)\n","        x = self.dropout1(x)\n","        return x\n","\n","    def forward(self, x):\n","        x = self._forward_features(x)\n","        x = self.flatten(x)\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout2(x)\n","        x = self.fc2(x)\n","        return x\n","\n","# 데이터 준비\n","data_dir = '/content/drive/MyDrive/살길찾기/rebalanced_dataset/'\n","dataset = AudioFolderDataset(data_dir, transform=preprocess)\n","\n","train_size = int(0.7 * len(dataset))\n","val_size = int(0.15 * len(dataset))\n","test_size = len(dataset) - train_size - val_size\n","train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n","\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n","\n","# 모델 초기화\n","input_shape = (1, 32, 32)  # 입력 크기를 (채널, 높이, 너비)로 설정\n","num_label = len(dataset.label_map)\n","model = CNNClassifier(input_shape, num_label)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# 훈련 루프\n","num_epochs = 20\n","best_val_accuracy = 0.0\n","best_model_path = 'best_model.pth'\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, labels in train_loader:\n","        inputs = inputs.unsqueeze(1)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n","\n","    # 평가\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            inputs = inputs.unsqueeze(1)\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","    val_accuracy = accuracy_score(all_labels, all_preds)\n","    print(f'Validation Accuracy: {val_accuracy}')\n","\n","    # 최적 모델 저장\n","    if val_accuracy > best_val_accuracy:\n","        best_val_accuracy = val_accuracy\n","        torch.save(model.state_dict(), best_model_path)\n","        print(f'Saved Best Model with Accuracy: {val_accuracy}')\n","\n","accuracy = accuracy_score(all_labels, all_preds)\n","print(f'Accuracy: {accuracy}')\n","\n"]},{"cell_type":"code","source":["# 모델 평가\n","model.load_state_dict(torch.load(best_model_path))\n","model.eval()\n","all_preds = []\n","all_labels = []\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs = inputs.unsqueeze(1)  # (batch_size, 1, 32, 32) 형태로 변환\n","        outputs = model(inputs)\n","        _, preds = torch.max(outputs, 1)\n","        all_preds.extend(preds.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","accuracy = accuracy_score(all_labels, all_preds)\n","print(f'Test Accuracy: {accuracy}')"],"metadata":{"id":"KpuBlvKGySPO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["일단 이코드로"],"metadata":{"id":"KcAsyaCFbWFc"}},{"cell_type":"code","source":["import os\n","import torch\n","import torchaudio\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from sklearn.metrics import accuracy_score\n","\n","# 폴더 구조를 기반으로 데이터셋 클래스를 정의\n","class AudioFolderDataset(Dataset):\n","    def __init__(self, root_dir, transform=None):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.file_paths = []\n","        self.labels = []\n","        self.label_map = {}\n","\n","        self._load_dataset()\n","\n","    def _load_dataset(self):\n","        label_names = os.listdir(self.root_dir)\n","        for idx, label_name in enumerate(label_names):\n","            label_dir = os.path.join(self.root_dir, label_name)\n","            if os.path.isdir(label_dir):\n","                self.label_map[label_name] = idx\n","                for file_name in os.listdir(label_dir):\n","                    if file_name.endswith('.wav'):\n","                        self.file_paths.append(os.path.join(label_dir, file_name))\n","                        self.labels.append(idx)\n","\n","    def __len__(self):\n","        return len(self.file_paths)\n","\n","    def __getitem__(self, idx):\n","        file_path = self.file_paths[idx]\n","        label = self.labels[idx]\n","        waveform, sample_rate = torchaudio.load(file_path)\n","        if self.transform:\n","            waveform = self.transform(waveform)\n","        return waveform, label\n","\n","# 전처리 함수 정의\n","def preprocess(waveform, new_sample_rate=8000):\n","    transform = torchaudio.transforms.Resample(orig_freq=waveform.shape[1], new_freq=new_sample_rate)\n","    return transform(waveform)\n","\n","# CNN 모델 정의\n","class CNNClassifier(nn.Module):\n","    def __init__(self, input_shape, num_label):\n","        super(CNNClassifier, self).__init__()\n","        self.norm_layer = nn.BatchNorm2d(1)\n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.dropout1 = nn.Dropout(0.25)\n","        self.flatten = nn.Flatten()\n","\n","        # CNN 레이어 이후 출력 크기 계산\n","        with torch.no_grad():\n","            sample_input = torch.zeros(1, *input_shape).unsqueeze(0)\n","            sample_output = self._forward_features(sample_input)\n","            flatten_size = sample_output.numel()\n","\n","        self.fc1 = nn.Linear(flatten_size, 128)\n","        self.dropout2 = nn.Dropout(0.5)\n","        self.fc2 = nn.Linear(128, num_label)\n","\n","    def _forward_features(self, x):\n","        x = F.interpolate(x, size=(32, 32), mode='bilinear', align_corners=False)\n","        x = self.norm_layer(x)\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = self.pool(x)\n","        x = self.dropout1(x)\n","        return x\n","\n","    def forward(self, x):\n","        x = self._forward_features(x)\n","        x = self.flatten(x)\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout2(x)\n","        x = self.fc2(x)\n","        return x\n","\n","# 데이터 준비\n","data_dir = '/content/drive/MyDrive/살길찾기/rebalanced_dataset/'  # 데이터 폴더 경로\n","dataset = AudioFolderDataset(data_dir, transform=preprocess)\n","\n","train_size = int(0.7 * len(dataset))\n","val_size = int(0.15 * len(dataset))\n","test_size = len(dataset) - train_size - val_size\n","train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n","\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n","\n","# 모델 초기화\n","input_shape = (1, 8000)  # 입력 크기를 (채널, 길이)로 설정\n","num_label = len(dataset.label_map)\n","model = CNNClassifier(input_shape, num_label)\n","\n","# 손실 함수 및 옵티마이저 정의\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# 모델 훈련 및 검증\n","num_epochs = 50\n","best_val_accuracy = 0.0\n","best_model_path = '/content/drive/MyDrive/살길찾기/model/model_1.pth'\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, labels in train_loader:\n","        inputs = inputs.unsqueeze(1)  # (batch_size, 1, 8000) 형태로 변환\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n","\n","    # 검증 루프\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            inputs = inputs.unsqueeze(1)  # (batch_size, 1, 8000) 형태로 변환\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    val_accuracy = accuracy_score(all_labels, all_preds)\n","    print(f'Validation Accuracy: {val_accuracy}')\n","\n","    # 최적 모델 저장\n","    if val_accuracy > best_val_accuracy:\n","        best_val_accuracy = val_accuracy\n","        torch.save(model.state_dict(), best_model_path)\n","        print(f'Saved Best Model with Accuracy: {val_accuracy}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ayet-kld0Rmx","executionInfo":{"status":"ok","timestamp":1723647912899,"user_tz":-540,"elapsed":192581,"user":{"displayName":"김태영","userId":"13282995900780911917"}},"outputId":"e9d137de-b8c1-43f8-c0a1-a7276dfe1d5a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 1.60326968299018\n","Validation Accuracy: 0.39285714285714285\n","Saved Best Model with Accuracy: 0.39285714285714285\n","Epoch 2, Loss: 1.3726365168889363\n","Validation Accuracy: 0.39285714285714285\n","Epoch 3, Loss: 1.2301290300157335\n","Validation Accuracy: 0.39285714285714285\n","Epoch 4, Loss: 1.0946548117531671\n","Validation Accuracy: 0.35714285714285715\n","Epoch 5, Loss: 1.029762367407481\n","Validation Accuracy: 0.42857142857142855\n","Saved Best Model with Accuracy: 0.42857142857142855\n","Epoch 6, Loss: 0.8639911744329665\n","Validation Accuracy: 0.42857142857142855\n","Epoch 7, Loss: 0.8250906334982978\n","Validation Accuracy: 0.6071428571428571\n","Saved Best Model with Accuracy: 0.6071428571428571\n","Epoch 8, Loss: 0.7488950888315836\n","Validation Accuracy: 0.5\n","Epoch 9, Loss: 0.6382901966571808\n","Validation Accuracy: 0.6071428571428571\n","Epoch 10, Loss: 0.6185900304052565\n","Validation Accuracy: 0.5714285714285714\n","Epoch 11, Loss: 0.46140480869346195\n","Validation Accuracy: 0.5714285714285714\n","Epoch 12, Loss: 0.4423432730966144\n","Validation Accuracy: 0.6071428571428571\n","Epoch 13, Loss: 0.3876437892516454\n","Validation Accuracy: 0.6071428571428571\n","Epoch 14, Loss: 0.32616445753309464\n","Validation Accuracy: 0.6428571428571429\n","Saved Best Model with Accuracy: 0.6428571428571429\n","Epoch 15, Loss: 0.37723713119824726\n","Validation Accuracy: 0.5714285714285714\n","Epoch 16, Loss: 0.3789776927895016\n","Validation Accuracy: 0.6428571428571429\n","Epoch 17, Loss: 0.6986052592595419\n","Validation Accuracy: 0.5714285714285714\n","Epoch 18, Loss: 0.31771664321422577\n","Validation Accuracy: 0.5357142857142857\n","Epoch 19, Loss: 0.3403582026561101\n","Validation Accuracy: 0.5357142857142857\n","Epoch 20, Loss: 0.2581441406574514\n","Validation Accuracy: 0.5\n","Epoch 21, Loss: 0.2510935090896156\n","Validation Accuracy: 0.6071428571428571\n","Epoch 22, Loss: 0.2190649995787276\n","Validation Accuracy: 0.6428571428571429\n","Epoch 23, Loss: 0.23283401131629944\n","Validation Accuracy: 0.6428571428571429\n","Epoch 24, Loss: 0.25032427575853133\n","Validation Accuracy: 0.5714285714285714\n","Epoch 25, Loss: 0.18580658775236872\n","Validation Accuracy: 0.6071428571428571\n","Epoch 26, Loss: 0.28652854615615475\n","Validation Accuracy: 0.6071428571428571\n","Epoch 27, Loss: 0.19349458316961923\n","Validation Accuracy: 0.6071428571428571\n","Epoch 28, Loss: 0.22976594418287277\n","Validation Accuracy: 0.5357142857142857\n","Epoch 29, Loss: 0.16792593627340263\n","Validation Accuracy: 0.6428571428571429\n","Epoch 30, Loss: 0.1424810317500184\n","Validation Accuracy: 0.5714285714285714\n","Epoch 31, Loss: 0.25611738570862347\n","Validation Accuracy: 0.5\n","Epoch 32, Loss: 0.14736061232785383\n","Validation Accuracy: 0.5357142857142857\n","Epoch 33, Loss: 0.09537868481129408\n","Validation Accuracy: 0.5357142857142857\n","Epoch 34, Loss: 0.11635212662319343\n","Validation Accuracy: 0.5357142857142857\n","Epoch 35, Loss: 0.17548893733570972\n","Validation Accuracy: 0.5714285714285714\n","Epoch 36, Loss: 0.13549180949727693\n","Validation Accuracy: 0.5714285714285714\n","Epoch 37, Loss: 0.10460456771155198\n","Validation Accuracy: 0.5714285714285714\n","Epoch 38, Loss: 0.1064340037604173\n","Validation Accuracy: 0.5\n","Epoch 39, Loss: 0.10550255245632595\n","Validation Accuracy: 0.5714285714285714\n","Epoch 40, Loss: 0.07201587574026133\n","Validation Accuracy: 0.5714285714285714\n","Epoch 41, Loss: 0.09302966531282032\n","Validation Accuracy: 0.5714285714285714\n","Epoch 42, Loss: 0.12278334755036566\n","Validation Accuracy: 0.5714285714285714\n","Epoch 43, Loss: 0.09209490474313498\n","Validation Accuracy: 0.5714285714285714\n","Epoch 44, Loss: 0.12300251093175676\n","Validation Accuracy: 0.5714285714285714\n","Epoch 45, Loss: 0.07997641567554739\n","Validation Accuracy: 0.6071428571428571\n","Epoch 46, Loss: 0.13502115088825425\n","Validation Accuracy: 0.6428571428571429\n","Epoch 47, Loss: 0.09349617030885485\n","Validation Accuracy: 0.6071428571428571\n","Epoch 48, Loss: 0.08636538298904067\n","Validation Accuracy: 0.5714285714285714\n","Epoch 49, Loss: 0.14934452074683374\n","Validation Accuracy: 0.5714285714285714\n","Epoch 50, Loss: 0.09206102788448334\n","Validation Accuracy: 0.6071428571428571\n"]}]},{"cell_type":"code","source":["best_model_path = '/content/drive/MyDrive/살길찾기/model/model_1.pth'\n","# 모델 평가\n","model.load_state_dict(torch.load(best_model_path))\n","model.eval()\n","all_preds = []\n","all_labels = []\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs = inputs.unsqueeze(1)  # (batch_size, 1, 8000) 형태로 변환\n","        outputs = model(inputs)\n","        _, preds = torch.max(outputs, 1)\n","        all_preds.extend(preds.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","accuracy = accuracy_score(all_labels, all_preds)\n","print(f'Test Accuracy: {accuracy}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M08hcSqZaAi0","executionInfo":{"status":"ok","timestamp":1723647917171,"user_tz":-540,"elapsed":1052,"user":{"displayName":"김태영","userId":"13282995900780911917"}},"outputId":"c094c431-035e-4483-e213-ad2b2293a90a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.6896551724137931\n"]}]},{"cell_type":"markdown","source":["아래는 일단 패스"],"metadata":{"id":"eLYVWyH1bRpG"}},{"cell_type":"code","source":["import os\n","import torch\n","import torchaudio\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from sklearn.metrics import accuracy_score\n","\n","# 데이터셋 클래스 정의\n","class AudioFolderDataset(Dataset):\n","    def __init__(self, root_dir, transform=None):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.file_paths = []\n","        self.labels = []\n","        self.label_map = {}\n","\n","        self._load_dataset()\n","\n","    def _load_dataset(self):\n","        label_names = os.listdir(self.root_dir)\n","        for idx, label_name in enumerate(label_names):\n","            label_dir = os.path.join(self.root_dir, label_name)\n","            if os.path.isdir(label_dir):\n","                self.label_map[label_name] = idx\n","                for file_name in os.listdir(label_dir):\n","                    if file_name.endswith('.wav'):\n","                        self.file_paths.append(os.path.join(label_dir, file_name))\n","                        self.labels.append(idx)\n","\n","    def __len__(self):\n","        return len(self.file_paths)\n","\n","    def __getitem__(self, idx):\n","        file_path = self.file_paths[idx]\n","        label = self.labels[idx]\n","        waveform, sample_rate = torchaudio.load(file_path)\n","        if self.transform:\n","            waveform = self.transform(waveform)\n","        return waveform, label\n","\n","# 전처리 함수 정의\n","def preprocess(waveform, new_sample_rate=8000):\n","    transform = torchaudio.transforms.Resample(orig_freq=waveform.shape[1], new_freq=new_sample_rate)\n","    return transform(waveform)\n","\n","def mel_spectrogram(waveform, sample_rate=8000, n_mels=64, n_fft=1024, hop_length=512):\n","    mel_spec_transform = torchaudio.transforms.MelSpectrogram(\n","        sample_rate=sample_rate,\n","        n_fft=n_fft,\n","        hop_length=hop_length,\n","        n_mels=n_mels\n","    )\n","    return mel_spec_transform(waveform)\n","\n","# CNN 모델 정의\n","class CNNClassifier(nn.Module):\n","    def __init__(self, input_shape, num_classes):\n","        super(CNNClassifier, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n","        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.dropout1 = nn.Dropout(0.25)\n","        self.dropout2 = nn.Dropout(0.5)\n","\n","        # Conv 레이어를 거친 후의 출력 크기를 계산하여 Flatten 레이어의 입력 크기로 설정\n","        self.flatten_size = self._get_flatten_size(input_shape)\n","\n","        # self.fc1 = nn.Linear(self.flatten_size, 256)\n","        self.fc1 = nn.Linear(2048, 256)  # fc1의 입력 크기를 15360으로 수정\n","        self.fc2 = nn.Linear(256, num_classes)\n","\n","    def _get_flatten_size(self, input_shape):\n","        with torch.no_grad():\n","            x = torch.zeros(1, *input_shape)\n","            x = self._forward_features(x)\n","            return x.view(1, -1).size(1)\n","\n","    def _forward_features(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = self.pool(F.relu(self.conv3(x)))\n","        x = self.dropout1(x)\n","        return x\n","\n","    def forward(self, x):\n","        x = self._forward_features(x)\n","        x = x.view(x.size(0), -1)  # Flatten\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout2(x)\n","        x = self.fc2(x)\n","        return x\n","\n","# 데이터 준비\n","# data_dir = '/content/drive/MyDrive/살길찾기/donateacry_corpus'\n","data_dir = '/content/drive/MyDrive/살길찾기/rebalanced_dataset/'  # 데이터 폴더 경로\n","dataset = AudioFolderDataset(data_dir, transform=preprocess)\n","\n","# 데이터셋을 멜-스펙트로그램으로 변환\n","class MelSpectrogramDataset(Dataset):\n","    def __init__(self, dataset, sample_rate=8000):\n","        self.dataset = dataset\n","        self.sample_rate = sample_rate\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        waveform, label = self.dataset[idx]\n","        mel_spec = mel_spectrogram(waveform, sample_rate=self.sample_rate)\n","        mel_spec = mel_spec.squeeze(0).unsqueeze(0)  # (1, n_mels, time) 형태로 변환\n","        return mel_spec, label\n","\n","mel_dataset = MelSpectrogramDataset(dataset)\n","\n","train_size = int(0.7 * len(mel_dataset))\n","val_size = int(0.15 * len(mel_dataset))\n","test_size = len(mel_dataset) - train_size - val_size\n","train_dataset, val_dataset, test_dataset = random_split(mel_dataset, [train_size, val_size, test_size])\n","\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n","\n","# 모델 초기화\n","input_shape = (1, 64, 126)  # (채널, n_mels, time)으로 설정\n","num_classes = len(dataset.label_map)\n","model = CNNClassifier(input_shape, num_classes)\n","\n","# 손실 함수 및 옵티마이저 정의\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# 모델 훈련 및 검증\n","num_epochs = 1000\n","best_val_accuracy = 0.0\n","best_model_path = '/content/drive/MyDrive/살길찾기/model/best_rebalanced_model_1.pth'\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, labels in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n","\n","    # 검증 루프\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    val_accuracy = accuracy_score(all_labels, all_preds)\n","    print(f'Validation Accuracy: {val_accuracy}')\n","\n","    # 최적 모델 저장\n","    if val_accuracy > best_val_accuracy:\n","        best_val_accuracy = val_accuracy\n","        torch.save(model.state_dict(), best_model_path)\n","        print(f'Saved Best Model with Accuracy: {val_accuracy}')\n","\n"],"metadata":{"id":"8ciO0xV21ckb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 평가\n","model.load_state_dict(torch.load(best_model_path))\n","model.eval()\n","all_preds = []\n","all_labels = []\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        outputs = model(inputs)\n","        _, preds = torch.max(outputs, 1)\n","        all_preds.extend(preds.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","accuracy = accuracy_score(all_labels, all_preds)\n","print(f'Test Accuracy: {accuracy}')"],"metadata":{"id":"sophKMOerKOJ"},"execution_count":null,"outputs":[]}]}