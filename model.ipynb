{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcAsyaCFbWFc"
   },
   "source": [
    "## 전체 학습 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4361,
     "status": "ok",
     "timestamp": 1724778699874,
     "user": {
      "displayName": "김태영",
      "userId": "13282995900780911917"
     },
     "user_tz": -540
    },
    "id": "OnLBqsHon9Zf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class AudioFolderDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, target_sample_rate=8000, target_length=8000):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "        self.label_map = {}\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.target_length = target_length\n",
    "\n",
    "        self._load_dataset()\n",
    "\n",
    "    def _load_dataset(self):\n",
    "        label_names = os.listdir(self.root_dir)\n",
    "        for idx, label_name in enumerate(label_names):\n",
    "            label_dir = os.path.join(self.root_dir, label_name)\n",
    "            if os.path.isdir(label_dir):\n",
    "                self.label_map[label_name] = idx\n",
    "                for file_name in os.listdir(label_dir):\n",
    "                    if file_name.endswith('.wav'):\n",
    "                        self.file_paths.append(os.path.join(label_dir, file_name))\n",
    "                        self.labels.append(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # 오디오 파일 로드\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        \n",
    "        # 항상 모노로 변환\n",
    "        if waveform.shape[0] > 1:  # 스테레오(2 채널)일 경우\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "        # 전처리 함수 적용\n",
    "        waveform = self.preprocess(waveform, sample_rate)\n",
    "        \n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "        \n",
    "        return waveform, label\n",
    "\n",
    "    def preprocess(self, waveform, sample_rate):\n",
    "        # 리샘플링\n",
    "        if sample_rate != self.target_sample_rate:\n",
    "            transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.target_sample_rate)\n",
    "            waveform = transform(waveform)\n",
    "\n",
    "        # 길이 맞추기\n",
    "        waveform = self.pad_or_trim_waveform(waveform)\n",
    "        \n",
    "        return waveform\n",
    "\n",
    "    def pad_or_trim_waveform(self, waveform):\n",
    "        current_length = waveform.shape[1]\n",
    "        if current_length > self.target_length:\n",
    "            # 잘라내기\n",
    "            waveform = waveform[:, :self.target_length]\n",
    "        elif current_length < self.target_length:\n",
    "            # 패딩\n",
    "            pad_length = self.target_length - current_length\n",
    "            waveform = F.pad(waveform, (0, pad_length))\n",
    "        \n",
    "        return waveform\n",
    "\n",
    "# CNN 모델 정의\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, input_shape, num_label):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.norm_layer = nn.BatchNorm2d(1)\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # CNN 레이어 이후 출력 크기 계산\n",
    "        with torch.no_grad():\n",
    "            sample_input = torch.zeros(1, *input_shape).unsqueeze(0)\n",
    "            sample_output = self._forward_features(sample_input)\n",
    "            flatten_size = sample_output.numel()\n",
    "\n",
    "        self.fc1 = nn.Linear(flatten_size, 128)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, num_label)\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = F.interpolate(x, size=(32, 32), mode='bilinear', align_corners=False)\n",
    "        x = self.norm_layer(x)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout1(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 데이터 준비\n",
    "data_dir = './dataset/'  # 데이터 폴더 경로\n",
    "# dataset = AudioFolderDataset(data_dir, transform=preprocess)\n",
    "dataset = AudioFolderDataset(root_dir=data_dir, target_sample_rate=8000, target_length=8000)\n",
    "\n",
    "# 시드 고정\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ayet-kld0Rmx"
   },
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "input_shape = (1, 8000)  # 입력 크기를 (채널, 길이)로 설정\n",
    "num_label = len(dataset.label_map)\n",
    "model = CNNClassifier(input_shape, num_label)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 모델 훈련 및 검증\n",
    "num_epochs = 150\n",
    "best_val_accuracy = 0.0\n",
    "best_model_path = './model/model_1.pth'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.unsqueeze(1)  # (batch_size, 1, 8000) 형태로 변환\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "    # 검증 루프\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.unsqueeze(1)  # (batch_size, 1, 8000) 형태로 변환\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f'Validation Accuracy: {val_accuracy}')\n",
    "\n",
    "    # 최적 모델 저장\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f'Saved Best Model with Accuracy: {val_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLYVWyH1bRpG"
   },
   "source": [
    "## Test 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 841,
     "status": "ok",
     "timestamp": 1724646146265,
     "user": {
      "displayName": "김태영",
      "userId": "13282995900780911917"
     },
     "user_tz": -540
    },
    "id": "M08hcSqZaAi0",
    "outputId": "d7a59c80-836a-4d53-dbd9-e1620563dde7"
   },
   "outputs": [],
   "source": [
    "best_model_path = './model/model_1.pth'\n",
    "# 이후 필요할 때 테스트 셋 선언\n",
    "\n",
    "# 모델 초기화\n",
    "input_shape = (1, 8000)  # 입력 크기를 (채널, 길이)로 설정\n",
    "num_label = len(dataset.label_map)\n",
    "model = CNNClassifier(input_shape, num_label)\n",
    "# 모델 평가\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.unsqueeze(1)  # (batch_size, 1, 8000) 형태로 변환\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvIJqIWXnH4D"
   },
   "source": [
    "## Inference 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R20LoVa92gz0"
   },
   "source": [
    "### mp4 -> wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 896,
     "status": "ok",
     "timestamp": 1724646152294,
     "user": {
      "displayName": "김태영",
      "userId": "13282995900780911917"
     },
     "user_tz": -540
    },
    "id": "50gGNKht2Eee",
    "outputId": "7a02fb43-b5f4-45d0-a24a-80532a3151b9"
   },
   "outputs": [],
   "source": [
    "# 모듈 설치:\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "def extract_audio_from_video(video_file_path, audio_file_path):\n",
    "    # mp4 등 비디오 파일 불러오기\n",
    "    video = VideoFileClip(video_file_path)\n",
    "\n",
    "    # 오디오를 추출하여 mp3 파일로 저장\n",
    "    video.audio.write_audiofile(audio_file_path, codec='pcm_s16le')\n",
    "\n",
    "video_file = './mp4_folder/example.mp4'  # 변환하고 싶은 비디오 파일의 경로\n",
    "audio_file = './query_set/query_audio.wav'  # 저장할 오디오 파일의 경로, 이름 지정\n",
    "\n",
    "extract_audio_from_video(video_file, audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 468,
     "status": "ok",
     "timestamp": 1724646599641,
     "user": {
      "displayName": "김태영",
      "userId": "13282995900780911917"
     },
     "user_tz": -540
    },
    "id": "vSJ41rDAnFg3",
    "outputId": "bb7a8252-1afe-4a1d-a2ff-1f41454670c2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 전처리 함수 재정의 (Inference 시 필요)\n",
    "def preprocess_inference(waveform, new_sample_rate=8000):\n",
    "    transform = torchaudio.transforms.Resample(orig_freq=waveform.shape[1], new_freq=new_sample_rate)\n",
    "    return transform(waveform)\n",
    "\n",
    "# 모델 클래스(CNNClassifier) 및 기타 필요한 클래스와 함수는 동일하게 유지\n",
    "\n",
    "# 모델 초기화 및 가중치 로드\n",
    "input_shape = (1, 8000)  # 입력 크기를 (채널, 길이)로 설정\n",
    "num_label = len(dataset.label_map)  # 학습 때 사용했던 label_map의 길이\n",
    "model = CNNClassifier(input_shape, num_label)\n",
    "model.load_state_dict(torch.load(best_model_path))  # 학습된 가중치 로드\n",
    "model.eval()  # 모델을 평가 모드로 전환\n",
    "\n",
    "# 레이블 맵 (숫자 -> 클래스 이름)\n",
    "label_map = {v: k for k, v in dataset.label_map.items()}\n",
    "print(label_map)\n",
    "# 단일 파일에 대한 추론 함수 정의\n",
    "def predict_audio_class(file_path):\n",
    "    # 오디오 파일 로드\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "\n",
    "    # 전처리 수행\n",
    "    waveform = preprocess_inference(waveform)\n",
    "    waveform = waveform.unsqueeze(0)  # 배치 차원 추가\n",
    "    waveform = waveform.unsqueeze(1)  # 채널 차원 추가 (batch_size, 1, sample_length)\n",
    "\n",
    "    # 모델을 사용하여 예측 수행\n",
    "    with torch.no_grad():\n",
    "        outputs = model(waveform)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # 예측된 클래스 반환\n",
    "    predicted_class = label_map[predicted.item()]\n",
    "    return predicted_class\n",
    "\n",
    "# 예시로 사용할 오디오 파일 경로\n",
    "# example_file = './query_set/query_audio.wav'\n",
    "# example_file = './dataset/Belly_pain/549a46d8-9c84-430e-ade8-97eae2bef787-1430130772174-1.7-m-48-bp_dup0.wav' # belly pain\n",
    "# example_file = './dataset/Needs_to_burp/5afc6a14-a9d8-45f8-b31d-c79dd87cc8c6-1430757039803-1.7-m-48-bu_dup0.wav' # burping\n",
    "# example_file = './dataset/Discomfort/7b0e160e-0505-459e-8ecb-304d7afae9d2-1437486974312-1.7-m-04-dc_dup0.wav' # discomfort\n",
    "# example_file = './dataset/Hungry/0c8f14a9-6999-485b-97a2-913c1cbf099c-1430760379259-1.7-m-26-hu.wav' # hungry\n",
    "example_file = './dataset/Tired/03ADDCFB-354E-416D-BF32-260CF47F7060-1433658024-1.1-f-04-ti_dup0.wav' # tired\n",
    "\n",
    "\n",
    "# 예측 수행\n",
    "predicted_class = predict_audio_class(example_file)\n",
    "print(f'The predicted class for the input audio file is: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOZEUwXXvjOK142byrF1kWj",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
